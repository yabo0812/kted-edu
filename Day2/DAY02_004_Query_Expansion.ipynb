{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  쿼리 확장 (Query Expansion)\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 벡터스토어 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소 로드 \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"db_korean_cosine_metadata\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 백터 검색기 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 retriever 초기화\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 쿼리 확장 (Query Expansion)\n",
    "\n",
    "1. **Query Reformulation**\n",
    "    - LLM을 사용하여 원래 질문을 다른 형태로 재작성하는 방식임\n",
    "    - 동의어 확장, 질문 명확화, 관련 키워드 추가 등 다양한 방식으로 쿼리를 변형함\n",
    "    - 검색의 다양성과 정확도를 향상시키는 특징이 있음\n",
    "\n",
    "1. **Multi Query** \n",
    "    - Retriever에 지정된 LLM을 활용하여 원본 쿼리를 확장하는 방법임\n",
    "    - 하나의 질문에 대해 다양한 관점과 표현으로 여러 개의 쿼리를 자동 생성함\n",
    "    - LLM의 생성 능력을 활용하여 검색의 다양성과 포괄성을 향상시키는 특징이 있음\n",
    "\n",
    "1. **Decomposition** \n",
    "    - 복잡한 질문을 여러 개의 단순한 하위 질문으로 분해하는 LEAST-TO-MOST PROMPTING 전략을 사용함\n",
    "    - 각각의 하위 질문에 대해 개별적으로 검색을 수행하여 더 정확한 답변을 도출함\n",
    "    - 복잡한 질문을 체계적으로 해결하면서 검색의 정확도를 높이는 특징이 있음\n",
    "\n",
    "1. **Step-Back Prompting**\n",
    "    - 주어진 구체적인 질문에서 한 걸음 물러나 더 일반적인 개념이나 배경을 먼저 검색함\n",
    "    - 더 넓은 맥락에서 점차 구체적인 답변으로 좁혀가는 방식을 사용함\n",
    "    - 복잡한 질문에 대해 더 포괄적이고 정확한 답변을 제공하는 특징이 있음\n",
    "\n",
    "1. **HyDE (Hypothetical Document Embedding)**\n",
    "    - 주어진 질문에 대해 가상의 이상적인 답변 문서를 LLM으로 생성함\n",
    "    - 생성된 가상 문서를 임베딩하여 이를 기반으로 실제 문서를 검색하는 방식임\n",
    "    - 질문의 맥락을 더 잘 반영한 검색이 가능한 특징이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) **Query Reformulation** \n",
    "\n",
    "- **Query Reformulation**은 **LLM**을 활용해 원본 질문을 다양한 형태로 재구성\n",
    "- **동의어 확장**과 **키워드 추가**를 통해 검색 쿼리의 범위를 확장\n",
    "- 모호한 질문을 **명확하게 구체화**하여 검색 정확도 향상\n",
    "- 하나의 질문에 대해 **다양한 변형 쿼리**를 생성하여 검색 커버리지 확대\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_rewrite.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/abs/2305.14283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 쿼리 리포뮬레이션을 위한 프롬프트 템플릿 정의\n",
    "reformulation_template = \"\"\"다음 질문을 검색 성능을 향상시키기 위해 다시 작성해주세요:\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "다음 방식으로 질문을 재작성하세요:\n",
    "1. 동의어 추가\n",
    "2. 더 구체적인 키워드 포함\n",
    "3. 관련된 개념 확장\n",
    "\n",
    "[재작성된 질문]\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿 생성\n",
    "prompt = ChatPromptTemplate.from_template(reformulation_template)\n",
    "\n",
    "# LLM 모델 초기화\n",
    "llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0)\n",
    "\n",
    "# 쿼리 리포뮬레이션 체인 생성\n",
    "reformulation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 체인 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "reformulated_query = reformulation_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "pprint(f\"리포뮬레이션된 쿼리: \\n{reformulated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리포뮬레이션된 쿼리로 검색\n",
    "retrieved_docs = chroma_k_retriever.invoke(reformulated_query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runnable 객체로 변환하여 검색기 생성 (LCEL)\n",
    "reformulation_retriever = reformulation_chain | chroma_k_retriever\n",
    "\n",
    "# 쿼리 리포뮬레이션 검색기 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = reformulation_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **Multi Query** \n",
    "\n",
    "- **Multi Query**는 **Retriever의 LLM**을 사용해 단일 질문을 다수의 쿼리로 확장\n",
    "- 원본 질문에 대해 **다양한 관점**과 **표현 방식**으로 쿼리 자동 생성\n",
    "- **LLM의 생성 능력**을 활용해 검색 범위를 자연스럽게 확장\n",
    "- 검색의 **다양성**과 **포괄성**이 향상되어 관련 문서 검색 확률 증가\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/multi-query.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "[출처] https://arxiv.org/abs/2411.13154"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) MultiQueryRetriever 활용`\n",
    "\n",
    "- https://python.langchain.com/docs/how_to/MultiQueryRetriever/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티 쿼리 생성\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM 모델 초기화 (멀티 쿼리 생성용)\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4.1-mini',\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# 기본 retriever를 이용한 멀티 쿼리 생성 \n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma_k_retriever, llm=llm\n",
    ")\n",
    "\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "retrieved_docs = multi_query_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) Custom Prompt 활용`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "\n",
    "# 출력 파서: LLM 결과를 질문 리스트로 변환\n",
    "class LineListOutputParser(BaseOutputParser[List[str]]):\n",
    "    \"\"\"Output parser for a list of lines.\"\"\"\n",
    "\n",
    "    def parse(self, text: str) -> List[str]:\n",
    "        \"\"\"Split the text into lines and remove empty lines.\"\"\"\n",
    "        return [line.strip() for line in text.strip().split(\"\\n\") if line.strip()]\n",
    "    \n",
    "\n",
    "# 쿼리 생성 프롬프트\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"Generate three different versions of the given user question to retrieve relevant documents from a vector database. The goal is to reframe the question from various perspectives to overcome limitations of distance-based similarity search.\n",
    "\n",
    "    The generated questions should have the following characteristics:\n",
    "    1. Maintain the core intent of the original question but use different expressions or viewpoints.\n",
    "    2. Include synonyms or related concepts where possible.\n",
    "    3. Slightly broaden or narrow the scope of the question to potentially include diverse relevant information.\n",
    "\n",
    "    Write each question on a new line and include only the questions.\n",
    "\n",
    "    [Original question]\n",
    "    {question}\n",
    "    \n",
    "    [Alternative questions]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# 멀티쿼리 체인 구성\n",
    "multiquery_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# 테스트 쿼리 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "result = multiquery_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"생성된 대안 질문들:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리 검색기 생성\n",
    "multi_query_custom_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever, # 기본 retriever\n",
    "    llm_chain=multiquery_chain,   # 멀티쿼리 체인\n",
    "    parser_key=\"lines\"            # \"lines\": 출력 파서의 키\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_custom_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) **Decomposition** \n",
    "\n",
    "- **단계별 분해 전략**을 통해 복잡한 질문을 작은 단위로 나누어 처리함\n",
    "- 각 하위 질문마다 **독립적인 검색 프로세스**를 진행하여 정확도를 향상시킴\n",
    "- **LEAST-TO-MOST PROMPTING**을 활용하여 체계적인 문제 해결 방식을 구현함\n",
    "- 복잡한 문제를 단순화하여 검색 효율성을 극대화하는 방법론\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_decomposition.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/pdf/2205.10625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to decompose the given input question into multiple sub-questions. \n",
    "    The goal is to break down the input into a set of sub-problems/sub-questions that can be answered independently.\n",
    "\n",
    "    Follow these guidelines to generate the sub-questions:\n",
    "    1. Cover various aspects related to the core topic of the original question.\n",
    "    2. Each sub-question should be specific, clear, and answerable independently.\n",
    "    3. Ensure that the sub-questions collectively address all important aspects of the original question.\n",
    "    4. Consider temporal aspects (past, present, future) where applicable.\n",
    "    5. Formulate the questions in a direct and concise manner.\n",
    "\n",
    "    [Input question] \n",
    "    {question}\n",
    "\n",
    "    [Sub-questions (5)]\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# 쿼리 생성 체인\n",
    "decomposition_chain = QUERY_PROMPT | llm | LineListOutputParser()\n",
    "\n",
    "# 테스트 쿼리 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "result = decomposition_chain.invoke({\"question\": query})\n",
    "\n",
    "print(\"생성된 서브 질문들:\")\n",
    "for i, q in enumerate(result, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 쿼리 검색기 생성\n",
    "multi_query_decompostion_retriever = MultiQueryRetriever(\n",
    "    retriever=chroma_k_retriever,    # 기본 retriever\n",
    "    llm_chain=decomposition_chain,   # 서브 질문 생성 체인\n",
    "    parser_key=\"lines\"               # \"lines\": 출력 파서의 키\n",
    ")  \n",
    "\n",
    "retrieved_docs = multi_query_decompostion_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) **Step-Back Prompting**\n",
    "\n",
    "- **단계적 후퇴 방식**을 통해 구체적 질문을 일반적 맥락에서 접근함\n",
    "- **맥락 기반 검색**으로 넓은 관점에서 구체적 답변으로 좁혀나감\n",
    "- **포괄적 접근법**을 활용하여 복잡한 질문에 대한 이해도를 높임\n",
    "- 일반적 맥락에서 시작하여 구체적 해답을 찾아가는 체계적 접근 방식\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_stepback.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/pdf/2310.06117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Step-Back 질문 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Few Shot 예제 - (구체적 질문, 포괄적 질문) 쌍\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"애플의 M1 칩 개발이 기업 가치에 미친 영향은?\",\n",
    "        \"output\": \"기업의 핵심 기술 내재화가 경쟁우위에 미치는 영향은 무엇인가?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"아마존의 AWS가 수익성에 기여하는 방식은?\",\n",
    "        \"output\": \"기업의 새로운 사업 영역 확장이 수익 구조에 미치는 영향은 무엇인가?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"토요타의 하이브리드 기술 전략의 핵심은?\",\n",
    "        \"output\": \"자동차 산업에서 친환경 기술 혁신이 기업 성장에 미치는 영향은 무엇인가?\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# 프롬프트 템플릿 초기화\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Step-Back 생성을 위한 프롬프트\n",
    "step_back_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\n",
    "                \"system\",\n",
    "                \"\"\"당신은 기업 분석 전문가입니다. 특정 기업에 대한 구체적인 질문을 해당 산업이나 비즈니스 전반의 일반적인 관점에서 \n",
    "                재해석하는 것이 임무입니다. 산업 동향, 경쟁 구도, 기술 혁신, 사업 모델 등의 관점에서 더 포괄적인 질문으로 \n",
    "                바꾸어 주세요. 다음은 예시입니다:\"\"\"\n",
    "            ),\n",
    "            few_shot_prompt,\n",
    "            (\"user\", \"{question}\"),\n",
    "        ])\n",
    "\n",
    "# Step-Back 체인 생성\n",
    "step_back_chain = step_back_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step-Back 질문 생성\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "step_back_question = step_back_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"Step-Back 질문: {step_back_question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-Back 검색기 생성\n",
    "step_back_retriever = step_back_chain | chroma_k_retriever\n",
    "\n",
    "# Step-Back 검색 실행\n",
    "retrieved_docs = step_back_retriever.invoke({\"question\": query})\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 최종 답변 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# 프롬프트 템플릿 초기화\n",
    "response_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"당신은 전문가입니다. 다음 컨텍스트와 질문을 바탕으로 포괄적인 답변을 제공해주세요.\n",
    "\n",
    "            일반 컨텍스트:\n",
    "            {normal_context}\n",
    "            \n",
    "            기본 개념 컨텍스트:\n",
    "            {step_back_context}\n",
    "            \n",
    "            원래 질문: {question}\n",
    "            \n",
    "            답변:\"\"\"\n",
    "        )\n",
    "\n",
    "# 문서 포맷팅 함수\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "\n",
    "# 답변 생성 체인\n",
    "answer_chain = (\n",
    "            {\n",
    "                \"normal_context\": chroma_k_retriever,\n",
    "                \"step_back_context\": step_back_retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | response_prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "# 답변 생성\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "answer = answer_chain.invoke(query)\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) **HyDE** (Hypothetical Document Embedding)\n",
    "\n",
    "- **가상 문서 생성**을 통해 주어진 질문에 대해 가상의 이상적인 답변 문서를 LLM으로 생성함\n",
    "- 생성된 문서의 **임베딩 기반 검색**으로 실제 문서와 매칭을 수행함\n",
    "- **맥락 기반 검색 방식**으로 질문의 의도를 더 정확하게 반영함\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/tsdata/image_files/main/202505/query_HyDE.png\" alt=\"rag\" align=\"center\" border=\"0\"  width=\"800\" height=auto>\n",
    "</center>\n",
    "\n",
    "\n",
    "[출처] https://arxiv.org/abs/2212.10496"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 가상 문서 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# HyDE를 위한 프롬프트 템플릿 생성\n",
    "template = \"\"\"주어진 질문에 대한 이상적인 문서 내용을 생성해주세요.\n",
    "문서는 학술적이고 전문적인 톤으로 작성되어야 합니다.\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "문서 내용:\"\"\"\n",
    "\n",
    "hyde_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# LLM 모델 초기화\n",
    "hyde_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# 문서 생성 체인 생성\n",
    "hyde_chain = hyde_prompt | hyde_llm | StrOutputParser()\n",
    "\n",
    "# 문서 생성 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "hypothetical_doc = hyde_chain.invoke({\"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"문서 내용: {hypothetical_doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 유사 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가상 문서를 기반으로 실제 문서 검색\n",
    "    \n",
    "retrieved_docs = chroma_k_retriever.invoke(hypothetical_doc)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) 최종 답변 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 RAG를 위한 프롬프트 템플릿 생성\n",
    "template = \"\"\"다음 컨텍스트를 바탕으로 질문에 답변해주세요:\n",
    "\n",
    "컨텍스트:\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "rag_prompt =  ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# RAG 체인 생성\n",
    "rag_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "rag_chain = rag_prompt | rag_llm | StrOutputParser()\n",
    "    \n",
    "# RAG 실행\n",
    "query = \"리비안의 사업 경쟁력은 어디서 나오나요?\"\n",
    "context = format_docs(retrieved_docs)\n",
    "\n",
    "answer = rag_chain.invoke({\"context\": context, \"question\": query})\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"답변: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) HyDE 체인 종합`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 가상 문서 생성\n",
    "query = \"테슬라의 경영진을 분석해주세요.\"\n",
    "\n",
    "hypothetical_doc = hyde_chain.invoke({\"question\": query})\n",
    "\n",
    "# Step 2. 유사 문서 검색\n",
    "retrieved_docs = chroma_k_retriever.invoke(hypothetical_doc)\n",
    "\n",
    "# Step 3. 최종 답변 생성\n",
    "final_answer = rag_chain.invoke(\n",
    "    {\n",
    "        \"context\": format_docs(retrieved_docs), \n",
    "        \"question\": query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"쿼리: {query}\")\n",
    "print(f\"가상 문서 내용: {hypothetical_doc}\")\n",
    "print(f\"답변: {final_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
