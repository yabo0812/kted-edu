{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  재순위화(Re-rank) 기법, 맥락 압축(Contextual Compression) 기법\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 벡터스토어 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 저장소 로드 \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "chroma_db = Chroma(\n",
    "    collection_name=\"db_korean_cosine_metadata\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 백터 검색기 생성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 retriever 초기화\n",
    "chroma_k_retriever = chroma_db.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")\n",
    "\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = chroma_k_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Re-rank** (재순위화)\n",
    "\n",
    "- **재순위화**는 검색 결과를 재분석하여 최적의 순서로 정렬하는 고도화된 기술임\n",
    "\n",
    "- **이중 단계 프로세스**로 기본 검색 후 정교한 기준으로 재평가를 진행함\n",
    "    1. 먼저 기본 검색 알고리즘으로 관련 문서들을 찾은 후, \n",
    "    2. 더 정교한 기준으로 이들을 재평가하여 최종 순위를 결정\n",
    "\n",
    "- 사용자의 검색 의도에 맞는 **정확도 향상**을 통해 검색 품질을 개선함\n",
    "\n",
    "- 검색 결과의 품질을 높이기 위한 체계적인 최적화 방법론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### 1) **Cross Encoder** Reranker\n",
    "\n",
    "- **Cross-Encoder** 모델을 활용하여 검색 결과의 정밀한 재정렬을 수행함\n",
    "- 데이터를 **쌍(pair) 단위**로 처리하여 문서와 쿼리 간의 관계를 분석함 (예: 두 개의 문장 또는 문서)\n",
    "- **통합 인코딩 방식**으로 검색 쿼리와 검색된 문서 간 유사도를 더 정확하게 계산함\n",
    "\n",
    "- 참고: https://www.sbert.net/examples/applications/cross-encoder/README.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 모델 초기화`\n",
    "\n",
    "- pip install hf_xet 또는 uv pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# CrossEncoderReranker 모델 초기화 \n",
    "model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\")\n",
    "\n",
    "# CrossEncoderReranker 모델을 사용한 re-ranker 초기화 (top_n: 3)\n",
    "re_ranker = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "# CrossEncoderReranker를 사용한 retriever 초기화\n",
    "cross_encoder_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=chroma_k_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEncoderReranker를 사용한 retriever를 사용하여 검색\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = cross_encoder_reranker_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **LLM** Reranker\n",
    "\n",
    "- **대규모 언어 모델**을 활용하여 검색 결과의 재순위화를 수행함\n",
    "- 쿼리와 문서 간의 **관련성 분석**을 통해 최적의 순서를 도출함\n",
    "- **LLMListwiseRerank**와 같은 전문화된 재순위화 모델을 적용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 모델 초기화`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMListwiseRerank\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# LLMListwiseRerank 모델 초기화 (top_n: 3)\n",
    "re_ranker = LLMListwiseRerank.from_llm(llm, top_n=3)\n",
    "\n",
    "# LLMListwiseRerank 모델을 사용한 re-ranker 초기화\n",
    "llm_reranker_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=re_ranker, \n",
    "    base_retriever=chroma_k_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMListwiseRerank 모델을 사용한 retriever를 사용하여 검색\n",
    "\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "retrieved_docs = llm_reranker_retriever.invoke(query)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Contextual Compression** (맥락적 압축)\n",
    "\n",
    "- **맥락적 압축 기술**은 검색된 문서를 그대로 반환하는 대신, 쿼리 관련 정보만을 선별적으로 추출함\n",
    "\n",
    "- **이중 구조 시스템**으로 기본 검색과 문서 압축 과정을 수행함\n",
    "    1. 기본 검색기(base retriever) \n",
    "    2. 문서 압축기(Document Compressor)\n",
    "\n",
    "- **효율적인 처리**를 통해 LLM 비용 절감과 응답 품질 향상을 달성함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) **LLMChainFilter**\n",
    "\n",
    "- **LLM 기반 필터링**으로 검색된 문서의 포함 여부를 결정함\n",
    "- **원본 유지 방식**으로 문서 내용의 변경 없이 선별 작업을 수행함\n",
    "- **선택적 필터링**을 통해 관련성 높은 문서만을 최종 반환함\n",
    "- 문서 원본을 보존하면서 관련성 기반의 스마트한 선별을 수행하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 모델 초기화`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# LLMChainFilter 모델 초기화\n",
    "context_filter = LLMChainFilter.from_llm(llm)\n",
    "\n",
    "# LLMChainFilter 모델을 사용한 retriever 초기화\n",
    "llm_filter_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=context_filter,                   # LLM 기반 압축기\n",
    "    base_retriever=chroma_k_retriever,                # 기본 검색기 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMListwiseRerank 모델을 사용한 retriever를 사용하여 검색\n",
    "\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "compressed_docs = llm_filter_compression_retriever.invoke(query)\n",
    "\n",
    "for doc in compressed_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) **LLMChainExtractor**\n",
    "\n",
    "- **LLM 기반 추출**로 문서에서 쿼리 관련 핵심 내용만을 선별함\n",
    "- **순차적 처리 방식**으로 각 문서를 검토하여 관련 정보를 추출함\n",
    "- **맞춤형 요약**을 통해 쿼리에 최적화된 압축 결과를 생성함\n",
    "- 쿼리 맥락에 따른 선별적 정보 추출로 효율적인 문서 압축을 실현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 모델 초기화`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "# LLMChainExtractor 모델 초기화\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# LLMChainExtractor 모델을 사용한 retriever 초기화\n",
    "llm_extractor_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,                                    # LLM 기반 압축기\n",
    "    base_retriever=cross_encoder_reranker_retriever,               # 기본 검색기 (Re-rank)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLMChainExtractor 모델을 사용한 retriever를 사용하여 검색\n",
    "\n",
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "compressed_docs = llm_extractor_compression_retriever.invoke(query)\n",
    "\n",
    "\n",
    "for doc in compressed_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) **EmbeddingsFilter**\n",
    "\n",
    "- **임베딩 기반 필터링**으로 문서와 쿼리 간 유사도를 계산함\n",
    "- **LLM 미사용 방식**으로 빠른 처리 속도와 비용 효율성을 확보함 (LLM 호출보다 저렴하고 빠른 옵션)\n",
    "- **유사도 기준 선별**을 통해 관련성 높은 문서만을 효과적으로 추출함\n",
    "- 경제적이고 신속한 임베딩 기반의 문서 필터링 기법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 모델 초기화`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "# 임베딩 기반 압축기 초기화\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.4)\n",
    "\n",
    "# 임베딩 기반 압축기를 사용한 retriever 초기화\n",
    "embed_filter_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter,                             # 임베딩 기반 압축기\n",
    "    base_retriever=cross_encoder_reranker_retriever,               # 기본 검색기 (Re-rank)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "compressed_docs = embed_filter_compression_retriever.invoke(query)\n",
    "\n",
    "for doc in compressed_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) **DocumentCompressorPipeline**\n",
    "\n",
    "- **파이프라인 구조**로 여러 압축기를 순차적으로 연결하여 처리함\n",
    "- **복합 변환 기능**으로 문서 분할 및 중복 제거 등 다양한 처리가 가능함\n",
    "- **유연한 확장성**을 통해 BaseDocumentTransformers 추가로 기능을 확장함\n",
    "- 다중 압축기를 연계하여 포괄적이고 효과적인 문서 처리를 구현하는 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 모델 초기화`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
    "\n",
    "\n",
    "# 임베딩 기반 필터 초기화 - 중복 제거\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "\n",
    "# 임베딩 기반 필터 초기화 - 유사도 기반 필터 (임베딩 유사도 0.4 이상)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.4)\n",
    "\n",
    "# Re-ranking 모델 초기화\n",
    "re_ranker = LLMListwiseRerank.from_llm(llm, top_n=2)\n",
    "\n",
    "# DocumentCompressorPipeline 초기화 (순차적으로 redundant_filter -> relevant_filter -> re_ranker 적용)\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[redundant_filter, relevant_filter, re_ranker]\n",
    ")\n",
    "\n",
    "# DocumentCompressorPipeline을 사용한 retriever 초기화\n",
    "pipeline_compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline_compressor,           # DocumentCompressorPipeline 기반 압축기\n",
    "    base_retriever=chroma_k_retriever,             # 기본 검색기\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 문서 검색`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"테슬라 트럭 모델이 있나요?\"\n",
    "compressed_docs = pipeline_compression_retriever.invoke(query)\n",
    "\n",
    "for doc in compressed_docs:\n",
    "    print(f\"{doc.page_content} [출처: {doc.metadata['source']}]\")\n",
    "    print(\"=\"*200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
