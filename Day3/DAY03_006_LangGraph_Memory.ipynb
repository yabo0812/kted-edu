{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   LangGraph 활용 - 메모리 추가\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) Langsmith tracing 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracing 여부를 확인 (true: langsmith 추척 활성화, false: langsmith 추척 비활성화)\n",
    "import os\n",
    "print(os.getenv('LANGSMITH_TRACING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 레스토랑 메뉴 도구 설정`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Chroma 인덱스 로드 \n",
    "menu_db = Chroma(\n",
    "    embedding_function=embeddings_model,   \n",
    "    collection_name=\"restaurant_menu\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "wine_db = Chroma(\n",
    "    embedding_function=embeddings_model,   \n",
    "    collection_name=\"restaurant_wine\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "\n",
    "# Tool 정의 \n",
    "@tool\n",
    "def search_menu(query: str, k: int = 2) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant menu information from the encrypted database.\n",
    "    Use this tool only for menu-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = menu_db.similarity_search(query, k=k)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 메뉴 정보를 찾을 수 없습니다.\")]\n",
    "\n",
    "@tool\n",
    "def search_wine(query: str, k: int = 2) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Securely retrieve and access authorized restaurant wine menu information from the encrypted database.\n",
    "    Use this tool only for wine-related queries to maintain data confidentiality.\n",
    "    \"\"\"\n",
    "    docs = wine_db.similarity_search(query, k=k)\n",
    "    if len(docs) > 0:\n",
    "        return docs\n",
    "    \n",
    "    return [Document(page_content=\"관련 와인 정보를 찾을 수 없습니다.\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **단기 메모리 (Short-term Memory)**\n",
    "\n",
    "- **체크포인트 (Checkpoints)**는 그래프 처리 과정의 상태를 저장하고 관리하는 시스템 (단기 메모리 제공)\n",
    "\n",
    "- **체크포인트**는 각 스텝에서 생성되는 그래프 상태의 **스냅샷**으로 구성\n",
    "\n",
    "    - `config`: 체크포인트와 관련된 설정.\n",
    "    - `metadata`: 체크포인트와 관련된 메타데이터.\n",
    "    - `values`: 해당 시점의 상태 채널 값.\n",
    "    - `next`: 다음에 실행할 노드 이름의 튜플.\n",
    "    - `tasks`: 다음에 실행할 작업에 대한 정보를 포함하는 `PregelTask` 객체의 튜플.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. **MemorySaver**\n",
    "\n",
    "- **MemorySaver**는 LangGraph에서 제공하는 스레드 기반의 단기 메모리(short-term memory)\n",
    "\n",
    "- **단기 메모리**는 하나의 **대화 세션** 동안만 정보를 유지\n",
    "\n",
    "- LangGraph는 **에이전트의 상태**로서 단기 메모리를 관리하며, 체크포인터를 통해 데이터베이스에 저장됨\n",
    "\n",
    "- 메모리는 그래프 실행 또는 단계 완료 시 **업데이트**되며, 각 단계 시작 시 상태를 읽어들임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1)  상태 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Optional\n",
    "from typing_extensions import TypedDict\n",
    "from operator import add\n",
    "\n",
    "# 상태 정의\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    search_results: Annotated[list[str], add]\n",
    "    summary: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 노드 정의`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# LLM에 도구를 바인딩 (2개의 도구 바인딩)\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 도구 노드 정의 \n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "# Summary 채인\n",
    "system_prompt = \"\"\"\n",
    "You are an AI assistant helping a user find information about a restaurant menu and wine list. \n",
    "Answer in the same language as the user's query.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Summarize the following search results.\n",
    "\n",
    "[GUIDELINES]\n",
    "- Provide a brief summary of the search results.\n",
    "- Include the key information from the search results.\n",
    "- Use 1-2 sentences to summarize the information.\n",
    "\n",
    "[Search Results]\n",
    "{search_results}\n",
    "\n",
    "[Summary]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", user_prompt),\n",
    "    ]\n",
    ")\n",
    "\n",
    "summary_chain = summary_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노드 정의 \n",
    "def search_node(state: State): \n",
    "    \"\"\"Performs a database search based on the query.\"\"\"\n",
    "    query = state['query']\n",
    "    \n",
    "    # 검색 도구 사용 \n",
    "    tool_call = llm_with_tools.invoke(query)\n",
    "    tool_results = tool_node.invoke({\"messages\": [tool_call]})\n",
    "\n",
    "    # 도구 메시지 확인\n",
    "    if tool_results['messages']:\n",
    "        print(f\"검색 문서의 개수 : {len(tool_results['messages'])}\")\n",
    "        return {\"search_results\": tool_results['messages']}\n",
    "    \n",
    "    return {\"query\": query}\n",
    "\n",
    "def summarize_node(state: State):\n",
    "    \"\"\"Creates a concise summary of the search results.\"\"\"\n",
    "    search_results = state['search_results']\n",
    "\n",
    "    summary_text = summary_chain.invoke({\"search_results\": search_results})\n",
    "    summary_text = str(summary_text.content).replace(\"\\n\", \" \").strip()\n",
    "    \n",
    "    summary = f\"Summary of results for '{state['query']}': \" + summary_text if search_results else \"No results found.\"\n",
    "    return {\"summary\": summary}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) StateGraph 구성`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# StateGraph 생성\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드 추가\n",
    "workflow.add_node(\"search\", search_node)\n",
    "workflow.add_node(\"summarize\", summarize_node)\n",
    "\n",
    "# 엣지 연결\n",
    "workflow.add_edge(START, \"search\")\n",
    "workflow.add_edge(\"search\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(4) 체크포인트 설정`\n",
    "\n",
    "- 그래프를 컴파일할 때 체크포인터를 지정\n",
    "- **MemorySaver**: 디버깅/테스트 용도로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 메모리 저장소 생성\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# 메모리 저장소를 지정하여 그래프 컴파일\n",
    "graph_memory = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(graph_memory.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(5) 체크포인터 사용`\n",
    "- 메모리 사용 시 `thread_id`를 지정 \n",
    "- 체크포인터는 그래프의 각 단계에서 상태를 기록 (그래프 각 단계의 모든 상태를 컬렉션으로 저장)\n",
    "- 나중에 `thread_id`를 사용하여 이 스레드에 접근 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "initial_input = {\"query\": \"스테이크 메뉴가 있나요? 어울리는 와인도 추천해주세요.\"}\n",
    "\n",
    "# 그래프 실행\n",
    "output = graph_memory.invoke(initial_input, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 결과 출력\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(6) 상태 가져오기`\n",
    "\n",
    "- `graph.get_state(config)`는 스레드의 **최신 상태**를 조회하는 메서드임\n",
    "\n",
    "- 상태 조회 시 필수적으로 **thread_id**를 지정해야 함\n",
    "\n",
    "- **checkpoint_id** 지정 시 특정 체크포인트 시점의 상태를 가져올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 상태 출력 (가장 최근 상태)\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "current_state = graph_memory.get_state(config)\n",
    "\n",
    "# 현재 상태의 속성 출력\n",
    "print(f\"config: {current_state.config}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"metadata: {current_state.metadata}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"next: {current_state.next}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"tasks: {current_state.tasks}\")\n",
    "print(\"-\" * 100)\n",
    "print(\"values:\")\n",
    "pprint(current_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(7) 상태 히스토리 가져오기`\n",
    "\n",
    "- `graph.get_state_history(config)`로 스레드의 **전체 실행 기록**을 조회함\n",
    "\n",
    "- 반환값은 **StateSnapshot 객체** 리스트 형태임\n",
    "\n",
    "- 리스트의 첫 번째 요소가 **가장 최근 체크포인트**를 나타냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 히스토리 출력\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "state_history = list(graph_memory.get_state_history(config))\n",
    "\n",
    "for i, state_snapshot in enumerate(state_history):\n",
    "    print(f\"  Checkpoint {i}:\")\n",
    "    print(f\"    Values: {state_snapshot.values}\")\n",
    "    print(f\"    Next: {state_snapshot.next}\")\n",
    "    print(f\"    Metadata: {state_snapshot.metadata}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(8) 상태 업데이트`\n",
    "\n",
    "- `graph.update_state(config, values, as_node=None)`로 그래프의 **상태를 직접 수정**함\n",
    "\n",
    "- **values** : 업데이트할 값을 지정함\n",
    "\n",
    "- **as_node** : 업데이트를 수행할 노드를 지정 (선택 사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약('summarize')이 처리되기 이전 시점의 체크포인트 찾기\n",
    "snapshot_before_summarize = None\n",
    "for state_snapshot in state_history:\n",
    "    if state_snapshot.next == ('summarize',):\n",
    "        snapshot_before_summarize = state_snapshot\n",
    "        break\n",
    "\n",
    "print(f\"Snapshot before summarize: {snapshot_before_summarize}\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"Config before summarize: {snapshot_before_summarize.config}\")\n",
    "print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약('summarize')이 처리되기 이전 시점의 체크포인트\n",
    "snapshot_before_summarize.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 노드 확인\n",
    "list(snapshot_before_summarize.metadata['writes'].keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 업데이트 -> 'search' 노드에서 '쿼리'를 수정하고 다시 실행\n",
    "update_input = {\"query\": \"메뉴 이름, 가격 정보만 간단하게 출력하세요.\"}\n",
    "\n",
    "graph_memory.update_state(\n",
    "    snapshot_before_summarize.config, \n",
    "    update_input, \n",
    ")\n",
    "\n",
    "updated_state = graph_memory.get_state(config)\n",
    "\n",
    "# 업데이트된 상태 출력\n",
    "pprint(updated_state.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 업데이트된 상태로 이어서 실행 (재생)\n",
    "graph_memory.invoke(None, config)\n",
    "\n",
    "# 업데이트된 상태의 히스토리 출력\n",
    "state_history_after_update = list(graph_memory.get_state_history(config))\n",
    "\n",
    "for i, state_snapshot in enumerate(state_history_after_update):\n",
    "    print(f\"  Checkpoint {i}:\")\n",
    "    print(f\"    Values: {state_snapshot.values}\")\n",
    "    print(f\"    Next: {state_snapshot.next}\")\n",
    "    print(f\"    Metadata: {state_snapshot.metadata}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 상태 출력\n",
    "final_state = graph_memory.get_state(config)\n",
    "\n",
    "pprint(final_state.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. **메시지 관리하기**\n",
    "\n",
    "- **긴 대화 기록**은 LLM의 컨텍스트 윈도우 제한으로 인한 오류나 성능 저하를 초래함\n",
    "\n",
    "- 메모리 관리는 **정확성**과 **응답 시간**, **비용** 사이의 균형이 필요함\n",
    "\n",
    "- 주요 해결책으로 **메시지 목록 편집**과 **과거 대화 요약**이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 긴 대화 관리 - 직접 메시지 삭제 방식`\n",
    "\n",
    "- **컨텍스트 제한**으로 인해 LLM이 처리할 수 있는 메시지 길이에 제약이 있음\n",
    "- 효율적인 **토큰 관리**를 통해 비용을 절감하고 시스템 성능을 최적화할 수 있음\n",
    "- 신속한 **응답 속도**를 위해 메시지 길이와 복잡성 조절이 필수적임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from typing import Union, Annotated\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# 메시지 관리자 정의 (리듀서)\n",
    "def manage_list(existing: list, updates: Union[list, dict]):\n",
    "\n",
    "    # 업데이트가 리스트인 경우: 기존 메시지에 추가\n",
    "    if isinstance(updates, list):\n",
    "        return existing + updates\n",
    "    \n",
    "    # 업데이트가 딕셔너리인 경우: 메시지 관리 작업 수행\n",
    "    elif isinstance(updates, dict) and updates[\"type\"] == \"keep\":\n",
    "\n",
    "        # 지정된 범위의 메시지만 선택 (from ~ to)\n",
    "        recent_messages = existing[updates[\"from\"]:updates[\"to\"]]\n",
    "        \n",
    "        # tool call과 response 쌍 찾기 + 일반 메시지 선별\n",
    "        kept_indices = set()\n",
    "        \n",
    "        for i, msg in enumerate(recent_messages):\n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                if i+1 < len(recent_messages) and isinstance(recent_messages[i+1], ToolMessage):\n",
    "                    # tool 쌍의 인덱스 저장\n",
    "                    kept_indices.add(i)\n",
    "                    kept_indices.add(i+1)\n",
    "            elif not isinstance(msg, ToolMessage):\n",
    "                # 일반 메시지 인덱스 저장\n",
    "                kept_indices.add(i)\n",
    "                \n",
    "        # 원본 순서 유지하면서 선택된 메시지만 반환\n",
    "        return [msg for i, msg in enumerate(recent_messages) if i in kept_indices]\n",
    "        \n",
    "    return existing\n",
    "\n",
    "\n",
    "# LangGraph MessagesState 사용\n",
    "class GraphState(MessagesState):\n",
    "    messages: Annotated[list, manage_list]  # 메시지 관리자 리듀서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메시지 관리 노드\n",
    "def message_manager(state: GraphState):\n",
    "    # 최근 5개 메시지만 유지\n",
    "    return {\n",
    "        \"messages\": {\"type\": \"keep\", \"from\": -5, \"to\": None}\n",
    "    }\n",
    "\n",
    "# LLM 모델에 도구를 바인딩 \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "\n",
    "# 에이전트 실행 노드 \n",
    "def call_model(state: GraphState):\n",
    "    system_prompt = SystemMessage(\"\"\"You are a helpful AI assistant. Please respond to the user's query to the best of your ability!\n",
    "\n",
    "중요: 답변을 제공할 때 반드시 정보의 출처를 명시해야 합니다. 출처는 다음과 같이 표시하세요:\n",
    "- 도구를 사용하여 얻은 정보: [도구: 도구이름]\n",
    "- 모델의 일반 지식에 기반한 정보: [일반 지식]\n",
    "\n",
    "항상 정확하고 관련성 있는 정보를 제공하되, 확실하지 않은 경우 그 사실을 명시하세요. 출처를 명확히 표시함으로써 사용자가 정보의 신뢰성을 판단할 수 있도록 해주세요.\"\"\")\n",
    "    \n",
    "    # 시스템 메시지와 이전 메시지를 결합하여 모델 호출\n",
    "    messages = [system_prompt] + state['messages']\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    # 메시지 리스트로 반환하고 상태 업데이트 \n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 구성\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "builder.add_node(\"agent\", call_model) \n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_node(\"message_manager\", message_manager)\n",
    "\n",
    "builder.add_edge(START, \"message_manager\")\n",
    "builder.add_edge(\"message_manager\", \"agent\")\n",
    "\n",
    "# tools_condition을 사용한 조건부 엣지 추가\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 메모리 저장소 생성\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 메모리 저장소를 지정하여 그래프 컴파일\n",
    "graph_memory_trimmer = builder.compile(checkpointer=memory)\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(graph_memory_trimmer.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 (초기 메시지 사용)\n",
    "messages = graph_memory_trimmer.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"둘 중에 더 저렴한 메뉴는 무엇인가요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 \n",
    "messages = graph_memory_trimmer.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"이 메뉴와 곁들이면 좋은 다른 메뉴가 있나요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력\n",
    "messages = graph_memory_trimmer.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 긴 대화 관리 - RemoveMessage를 사용한 메시지 삭제 방식`\n",
    "\n",
    "- **메시지 삭제 방식**은 삭제할 메시지의 ID를 지정하는 \"remove\" 객체 목록을 반환함\n",
    "- **LangGraph**의 `MessagesState`는 `RemoveMessage` 기능을 활용하여 메시지를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage, RemoveMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# LangGraph MessagesState 사용\n",
    "class GraphState(MessagesState):\n",
    "    ...\n",
    "\n",
    "# LLM 모델에 도구를 바인딩 \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "\n",
    "# 노드 함수 정의\n",
    "def call_model(state: GraphState):\n",
    "    system_prompt = SystemMessage(\"\"\"You are a helpful AI assistant. Please respond to the user's query to the best of your ability!\n",
    "\n",
    "    중요: 답변을 제공할 때 반드시 정보의 출처를 명시해야 합니다. 출처는 다음과 같이 표시하세요:\n",
    "    - 도구를 사용하여 얻은 정보: [도구: 도구이름]\n",
    "    - 모델의 일반 지식에 기반한 정보: [일반 지식]\n",
    "\n",
    "    항상 정확하고 관련성 있는 정보를 제공하되, 확실하지 않은 경우 그 사실을 명시하세요. 출처를 명확히 표시함으로써 사용자가 정보의 신뢰성을 판단할 수 있도록 해주세요.\"\"\")\n",
    "    \n",
    "    # 시스템 메시지와 이전 메시지를 결합하여 모델 호출\n",
    "    response = llm_with_tools.invoke([system_prompt]+state['messages'])\n",
    "\n",
    "    messages = state['messages']\n",
    "    keep_count = 3  # 유지할 최근 메시지 개수\n",
    "    \n",
    "    # 삭제할 메시지 인덱스 계산\n",
    "    delete_up_to = max(0, len(messages) - keep_count)\n",
    "    \n",
    "    # 삭제할 메시지들의 인덱스 조정 (도구 호출/응답 쌍 고려)\n",
    "    if delete_up_to > 0:\n",
    "        # 삭제 지점이 도구 응답 메시지라면 도구 호출도 함께 삭제되도록 인덱스 조정\n",
    "        if (delete_up_to < len(messages) and \n",
    "            isinstance(messages[delete_up_to], ToolMessage)):\n",
    "            delete_up_to -= 1\n",
    "            \n",
    "        # 반대로 삭제 지점이 도구 호출이라면 도구 응답도 함께 삭제되도록 인덱스 조정  \n",
    "        elif (delete_up_to < len(messages) and\n",
    "                hasattr(messages[delete_up_to], 'tool_calls') and \n",
    "                messages[delete_up_to].tool_calls and\n",
    "                delete_up_to + 1 < len(messages) and\n",
    "                isinstance(messages[delete_up_to + 1], ToolMessage)):\n",
    "            delete_up_to += 1\n",
    "\n",
    "    # 삭제할 메시지들의 RemoveMessage 생성\n",
    "    delete_messages = [\n",
    "        RemoveMessage(id=msg.id) \n",
    "        for msg in messages[:delete_up_to]\n",
    "    ]\n",
    "    \n",
    "    # 새로운 응답 메시지와 함께 반환\n",
    "    return {\"messages\": delete_messages + [response]}\n",
    "\n",
    "\n",
    "# 그래프 구성\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_edge(START, \"agent\")\n",
    "\n",
    "# tools_condition을 사용한 조건부 엣지 추가\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 메모리 저장소 생성\n",
    "memory = MemorySaver()\n",
    "\n",
    "# 메모리 저장소를 지정하여 그래프 컴파일\n",
    "graph_memory_remover = builder.compile(checkpointer=memory)\n",
    "\n",
    "# 그래프 출력\n",
    "display(Image(graph_memory_remover.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 (초기 메시지 사용)\n",
    "messages = graph_memory_remover.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "messages = [HumanMessage(content=\"둘 중에 더 저렴한 메뉴는 무엇인가요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력\n",
    "messages = graph_memory_remover.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thred_id 설정 유지한 상태에서 다른 메시지로 그래프 실행\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [HumanMessage(content=\"이 메뉴와 곁들이면 좋은 다른 메뉴가 있나요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력\n",
    "messages = graph_memory_remover.invoke({\"messages\": messages}, config)\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **장기 메모리 (Long-term Memory)**\n",
    "\n",
    "- **LangGraph 시스템**은 다중 세션에서 정보를 유지하는 장기 메모리 기능을 제공\n",
    "\n",
    "- 정보는 **JSON 문서** 형태로 저장되며, 사용자가 정의한 **네임스페이스**와 고유 키로 구성\n",
    "\n",
    "- 메모리 저장소는 **InMemoryStore** 또는 **DB 기반** 시스템을 선택하여 구현 가능\n",
    "\n",
    "- **콘텐츠 필터**를 통해 여러 네임스페이스 간의 효율적인 검색 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. **InMemoryStore**\n",
    "\n",
    "- LangGraph에서 **스레드(대화) 간에 정보를 공유**하고 저장하기 위한 인터페이스\n",
    "\n",
    "- **namespace 기반 저장**: 메모리는 튜플 형태의 namespace로 구분되어 저장 (예: (user_id, \"memories\"))\n",
    "\n",
    "- **key-value 저장**: 각 메모리는 고유한 key와 dictionary 형태의 value로 저장\n",
    "\n",
    "- **시맨틱 검색 지원**: 임베딩 모델을 사용하여 의미 기반 검색이 가능\n",
    "\n",
    "- **체크포인터와 연동**: 그래프의 체크포인터와 함께 동작하여 스레드 간 정보 공유 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) 기본 사용법`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "\n",
    "# InMemoryStore 생성\n",
    "store = InMemoryStore()\n",
    "\n",
    "# namespace 정의 \n",
    "user_id = \"1\"\n",
    "namespace = (user_id, \"memories\")\n",
    "\n",
    "# 메모리 저장\n",
    "memory_id = str(uuid.uuid4())\n",
    "memory = {\n",
    "    \"food_preference\": \"김치찌개를 좋아합니다\",\n",
    "    \"hobby\": \"등산\"\n",
    "}\n",
    "store.put(namespace, memory_id, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 검색\n",
    "memories = store.search(namespace)\n",
    "\n",
    "# 검색된 메모리 출력\n",
    "for memory in memories:\n",
    "    pprint(memory.dict())\n",
    "    print(\"-\" * 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) 시맨틱 검색 구현`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 임베딩 함수 정의\n",
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    return embeddings_model.embed_documents(texts)\n",
    "\n",
    "# 임베딩 모델을 사용하는 store 생성\n",
    "semantic_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embed,  # 임베딩 함수 지정\n",
    "        \"dims\": 1536,    # 임베딩 차원 지정\n",
    "        \"fields\": [\"food_preference\", \"hobby\"]  # 임베딩할 필드 지정\n",
    "    }  #type:ignore\n",
    ")  \n",
    "\n",
    "# 여러 메모리 저장\n",
    "memories_to_store = [\n",
    "    {\n",
    "        \"food_preference\": \"매운 음식을 좋아합니다\",\n",
    "        \"hobby\": \"영화 감상\"\n",
    "    },\n",
    "    {\n",
    "        \"food_preference\": \"한식을 선호합니다\",\n",
    "        \"hobby\": \"등산과 캠핑\" \n",
    "    },\n",
    "    {\n",
    "        \"food_preference\": \"양식을 좋아합니다\",\n",
    "        \"hobby\": \"요리\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for memory in memories_to_store:\n",
    "    memory_id = str(uuid.uuid4())\n",
    "    semantic_store.put(namespace, memory_id, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시맨틱 검색 수행\n",
    "search_results = semantic_store.search(\n",
    "    namespace,\n",
    "    query=\"캠핑에 어울리는 영화\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "for result in search_results:\n",
    "    print(f\"검색 결과: {result.dict()['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. **체크포인트 연동**\n",
    "\n",
    "- **체크포인터와 연동**: 그래프의 체크포인터와 함께 동작하여 스레드 간 정보 공유 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.store.base import BaseStore\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "from dataclasses import dataclass\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 메모리 저장소에서 사용할 네임스페이스 정의\n",
    "@dataclass \n",
    "class Namespace:\n",
    "    user_id: str\n",
    "    memory_type: str\n",
    "    \n",
    "    def to_tuple(self) -> tuple:\n",
    "        return (self.user_id, self.memory_type)\n",
    "\n",
    "# MessagesState를 확장하여 메모리 관련 필드 추가\n",
    "class GraphState(MessagesState):\n",
    "    summary: str  # 대화 요약을 저장할 필드\n",
    "\n",
    "# 메모리 업데이트 노드\n",
    "def update_memory(state: GraphState, config: RunnableConfig, *, store: BaseStore):\n",
    "    # 네임스페이스 동적 생성\n",
    "    namespace = Namespace(\n",
    "        user_id=config.get(\"configurable\", {}).get(\"user_id\", \"default\"),\n",
    "        memory_type=config.get(\"configurable\", {}).get(\"memory_type\", \"conversation_memory\")\n",
    "    )\n",
    "    \n",
    "    # 마지막 메시지 추출 (사용자의 대화 내용)\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    memory = {\n",
    "        \"conversation\": last_message.content,\n",
    "        \"timestamp\": str(datetime.now()),\n",
    "        \"type\": last_message.type\n",
    "    }\n",
    "    \n",
    "    # 메모리 저장\n",
    "    store.put(namespace.to_tuple(), str(uuid.uuid4()), memory)\n",
    "    return state\n",
    "\n",
    "\n",
    "# LLM 모델에 도구를 바인딩 \n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "tools = [search_menu, search_wine]\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "# LLM 호출 노드\n",
    "def call_model(state: GraphState, config: RunnableConfig, *, store: BaseStore):\n",
    "    system_prompt = SystemMessage(\"\"\"You are a helpful AI assistant...\"\"\")\n",
    "    \n",
    "    # 네임스페이스 동적 생성\n",
    "    namespace = Namespace(\n",
    "        user_id=config.get(\"configurable\", {}).get(\"user_id\", \"default\"),\n",
    "        memory_type=config.get(\"configurable\", {}).get(\"memory_type\", \"conversation_memory\")\n",
    "    )\n",
    "    \n",
    "    # 메모리 검색\n",
    "    memories = store.search(\n",
    "        namespace.to_tuple(),\n",
    "        query=state[\"messages\"][-1].content,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    # 이전 대화 메모리가 있는 경우, 메모리 내용을 요약에 추가\n",
    "    if memories:\n",
    "        memory_context = \"\\n이전 관련 대화:\\n\" + \"\\n\".join(\n",
    "            f\"- {m.value['conversation']}\" for m in memories\n",
    "        )\n",
    "        context_message = SystemMessage(content=memory_context)\n",
    "        messages = [system_prompt, context_message] + state[\"messages\"]\n",
    "\n",
    "    # 이전 대화 메모리가 없는 경우, 이전 메시지만 사용\n",
    "    else:\n",
    "        messages = [system_prompt] + state[\"messages\"]\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# 그래프 구성\n",
    "builder = StateGraph(GraphState)\n",
    "\n",
    "# 노드 추가\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.add_node(\"memory\", update_memory)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# 엣지 구성\n",
    "builder.add_edge(START, \"agent\")\n",
    "builder.add_edge(\"agent\", \"memory\")\n",
    "builder.add_conditional_edges(\n",
    "    \"memory\",\n",
    "    tools_condition\n",
    ")\n",
    "builder.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "\n",
    "# 임베딩 함수 정의\n",
    "def embed(texts: list[str]) -> list[list[float]]:\n",
    "    embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    return embeddings_model.embed_documents(texts)\n",
    "\n",
    "# 임베딩 모델을 사용하는 store 생성\n",
    "conversation_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embed,  # 임베딩 함수 지정\n",
    "        \"dims\": 1536,    # 임베딩 차원 지정\n",
    "        \"fields\": [\"conversation\"]  # 임베딩할 필드 지정\n",
    "    }  #type:ignore\n",
    ")  \n",
    "\n",
    "# 그래프 컴파일\n",
    "graph_with_store = builder.compile(\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=conversation_store  \n",
    ")\n",
    "\n",
    "# 그래프 시각화\n",
    "display(Image(graph_with_store.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 실행 \n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"thread_1\",\n",
    "        \"user_id\": \"user_123\",\n",
    "        \"memory_type\": \"conversation_memory\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴의 가격은 얼마인가요\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 (초기 메시지 사용)\n",
    "messages = graph_with_store.invoke({\"messages\": messages}, config) \n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스토어 검색 수행\n",
    "\n",
    "namespace = (\"user_123\", \"conversation_memory\")\n",
    "\n",
    "memories = conversation_store.search(\n",
    "    namespace,\n",
    "    query=\"샐러드\",\n",
    "    limit=2\n",
    ")\n",
    "\n",
    "# 검색된 메모리 출력\n",
    "for memory in memories:\n",
    "    pprint(memory.dict())\n",
    "    print(\"-\" * 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 스레드 ID로 그래프 실행\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": \"thread_2\",\n",
    "        \"user_id\": \"user_123\",\n",
    "        \"memory_type\": \"conversation_memory\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 초기 메시지 설정\n",
    "messages = [HumanMessage(content=\"스테이크 메뉴 가격이 얼마라고 했나요? 더 저렴한 메뉴는 무엇인가요?\")]\n",
    "\n",
    "# 그래프 실행 및 결과 출력 (초기 메시지 사용)\n",
    "messages = graph_with_store.invoke({\"messages\": messages}, config)\n",
    "\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
